"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.businessIntelligenceAggregator = exports.BusinessIntelligenceAggregator = exports.handler = exports.businessIntelligenceHandler = void 0;
const logger_service_1 = require("../shared/logger.service");
const database_service_1 = require("../shared/database.service");
const response_utils_1 = require("../shared/response.utils");
const lambda_auth_middleware_1 = require("../../shared/middleware/lambda-auth.middleware");
const redis_service_1 = require("../../services/redis.service");
const production_monitoring_service_1 = require("../../lib/monitoring/production-monitoring.service");
const zod_1 = require("zod");
const dataAggregationQuerySchema = zod_1.z.object({
    dimensions: zod_1.z.array(zod_1.z.enum(['time', 'school', 'region', 'subscription_tier', 'user_type', 'meal_type'])),
    measures: zod_1.z.array(zod_1.z.enum(['revenue', 'orders', 'students', 'satisfaction', 'efficiency', 'growth'])),
    timeGranularity: zod_1.z.enum(['hour', 'day', 'week', 'month', 'quarter', 'year']).default('day'),
    dateRange: zod_1.z.object({
        startDate: zod_1.z.string().datetime(),
        endDate: zod_1.z.string().datetime(),
    }),
    filters: zod_1.z.record(zod_1.z.string(), zod_1.z.any()).optional(),
    aggregationType: zod_1.z.enum(['sum', 'avg', 'count', 'min', 'max', 'distinct']).default('sum'),
    includeComparisons: zod_1.z.boolean().default(true),
    includeForecasts: zod_1.z.boolean().default(false),
});
const dataProcessingSchema = zod_1.z.object({
    operation: zod_1.z.enum(['etl_process', 'data_quality_check', 'cube_rebuild', 'lineage_analysis']),
    sourceType: zod_1.z.enum(['transactional', 'operational', 'external', 'streaming']),
    targetSchema: zod_1.z.string().optional(),
    processingMode: zod_1.z.enum(['batch', 'streaming', 'hybrid']).default('batch'),
    qualityRules: zod_1.z
        .array(zod_1.z.object({
        rule: zod_1.z.string(),
        threshold: zod_1.z.number(),
        action: zod_1.z.enum(['warn', 'reject', 'fix']),
    }))
        .optional(),
});
class BusinessIntelligenceAggregator {
    database;
    logger;
    redis;
    monitoring;
    cubeCache;
    aggregationCache;
    etlProcesses;
    MAX_CACHE_SIZE = 100;
    CACHE_TTL_MS = 30 * 60 * 1000;
    REDIS_CACHE_TTL = 15 * 60;
    cleanupInterval = null;
    constructor() {
        this.database = database_service_1.DatabaseService;
        this.logger = logger_service_1.LoggerService.getInstance();
        this.redis = redis_service_1.RedisService.getInstance();
        this.monitoring = new production_monitoring_service_1.ProductionMonitoringService({
            responseTime: { warning: 2000, critical: 5000 },
            errorRate: { warning: 5, critical: 15 },
            memoryUsage: { warning: 80, critical: 95 },
            cacheHitRate: { warning: 60, critical: 40 },
        });
        this.cubeCache = new Map();
        this.aggregationCache = new Map();
        this.etlProcesses = new Map();
        this.initializeDataCubes();
        this.initializeETLProcesses();
        this.startCacheCleanup();
    }
    startCacheCleanup() {
        this.cleanupInterval = setInterval(() => {
            this.cleanupExpiredCache();
        }, 5 * 60 * 1000);
        process.on('SIGTERM', () => this.cleanup());
        process.on('SIGINT', () => this.cleanup());
    }
    cleanupExpiredCache() {
        const now = Date.now();
        let cleanedCount = 0;
        for (const [key, entry] of this.aggregationCache.entries()) {
            if (now - entry.timestamp > this.CACHE_TTL_MS) {
                this.aggregationCache.delete(key);
                cleanedCount++;
            }
        }
        if (this.aggregationCache.size > this.MAX_CACHE_SIZE) {
            const entries = Array.from(this.aggregationCache.entries()).sort((a, b) => a[1].accessCount - b[1].accessCount);
            const toRemove = entries.slice(0, this.aggregationCache.size - this.MAX_CACHE_SIZE);
            for (const [key] of toRemove) {
                this.aggregationCache.delete(key);
                cleanedCount++;
            }
        }
        if (cleanedCount > 0) {
            this.logger.info('Cache cleanup completed', {
                cleanedCount,
                remainingSize: this.aggregationCache.size,
            });
        }
    }
    generateCacheKey(dimensions, measures, timeGranularity, dateRange, filters, includeComparisons) {
        const keyData = {
            dimensions: dimensions.sort(),
            measures: measures.sort(),
            timeGranularity,
            dateRange: {
                start: dateRange.startDate.toISOString(),
                end: dateRange.endDate.toISOString(),
            },
            filters,
            includeComparisons,
        };
        return `agg:${Buffer.from(JSON.stringify(keyData))
            .toString('base64')
            .replace(/[^a-zA-Z0-9]/g, '')
            .substring(0, 32)}`;
    }
    async getCachedAggregation(cacheKey) {
        try {
            const cachedData = await this.redis.get(cacheKey);
            if (cachedData) {
                const parsed = JSON.parse(cachedData);
                this.logger.info('Redis cache hit for aggregation', { cacheKey });
                return parsed;
            }
        }
        catch (error) {
            this.logger.warn('Redis cache read failed, falling back to database', {
                errorMessage: error instanceof Error ? error.message : String(error),
            });
        }
        return null;
    }
    async cacheAggregationResult(cacheKey, data) {
        try {
            await this.redis.set(cacheKey, JSON.stringify(data), this.REDIS_CACHE_TTL);
            this.logger.info('Aggregation result cached in Redis', {
                cacheKey,
                ttl: this.REDIS_CACHE_TTL,
            });
        }
        catch (error) {
            this.logger.warn('Redis cache write failed', {
                errorMessage: error instanceof Error ? error.message : String(error),
            });
        }
    }
    getCachedAggregationLocal(aggregationId) {
        const entry = this.aggregationCache.get(aggregationId);
        if (entry && Date.now() - entry.timestamp <= this.CACHE_TTL_MS) {
            entry.accessCount++;
            return entry.data;
        }
        if (entry) {
            this.aggregationCache.delete(aggregationId);
        }
        return null;
    }
    cleanup() {
        if (this.cleanupInterval) {
            clearInterval(this.cleanupInterval);
            this.cleanupInterval = null;
        }
        this.cubeCache.clear();
        this.aggregationCache.clear();
        this.etlProcesses.clear();
        this.logger.info('BusinessIntelligenceAggregator cleanup completed');
    }
    initializeDataCubes() {
        const revenueCube = {
            cubeId: 'revenue_analytics',
            name: 'Revenue Analytics Cube',
            description: 'Multi-dimensional revenue analysis with time, geography, and product dimensions',
            dimensions: [
                {
                    dimensionId: 'time',
                    name: 'Time',
                    type: 'temporal',
                    hierarchy: ['year', 'quarter', 'month', 'week', 'day'],
                    attributes: { timezone: 'Asia/Kolkata', calendar: 'gregorian' },
                    cardinality: 365,
                    uniqueValues: [],
                },
                {
                    dimensionId: 'school',
                    name: 'School',
                    type: 'categorical',
                    hierarchy: ['region', 'city', 'school'],
                    attributes: { tier: 'subscription_tier', type: 'school_type' },
                    cardinality: 250,
                    uniqueValues: [],
                },
                {
                    dimensionId: 'subscription_tier',
                    name: 'Subscription Tier',
                    type: 'categorical',
                    hierarchy: ['tier'],
                    attributes: { pricing: 'tier_pricing', features: 'tier_features' },
                    cardinality: 4,
                    uniqueValues: [
                        { value: 'basic', label: 'Basic', frequency: 45 },
                        { value: 'standard', label: 'Standard', frequency: 35 },
                        { value: 'premium', label: 'Premium', frequency: 15 },
                        { value: 'enterprise', label: 'Enterprise', frequency: 5 },
                    ],
                },
            ],
            measures: [
                {
                    measureId: 'total_revenue',
                    name: 'Total Revenue',
                    dataType: 'currency',
                    aggregationType: 'sum',
                    unit: 'INR',
                    format: 'â‚¹#,##0.00',
                    businessRules: ['Non-negative', 'Completed payments only'],
                },
                {
                    measureId: 'order_count',
                    name: 'Order Count',
                    dataType: 'integer',
                    aggregationType: 'count',
                    unit: 'orders',
                    format: '#,##0',
                    businessRules: ['Distinct order IDs'],
                },
            ],
            factTable: 'payment_facts',
            refreshFrequency: 'hourly',
            lastRefresh: new Date(),
            dataQuality: {
                completeness: 98.5,
                accuracy: 96.8,
                consistency: 97.2,
                timeliness: 99.1,
                validity: 98.0,
                overall: 97.9,
            },
            size: {
                rows: 2500000,
                compressed: 1024 * 1024 * 150,
                uncompressed: 1024 * 1024 * 450,
            },
            partitioning: {
                strategy: 'time',
                columns: ['transaction_date'],
                partitions: 12,
            },
        };
        this.cubeCache.set('revenue_analytics', revenueCube);
        const operationalCube = {
            cubeId: 'operational_analytics',
            name: 'Operational Analytics Cube',
            description: 'Operational efficiency and performance metrics across all dimensions',
            dimensions: [
                {
                    dimensionId: 'time',
                    name: 'Time',
                    type: 'temporal',
                    hierarchy: ['year', 'quarter', 'month', 'week', 'day', 'hour'],
                    attributes: { timezone: 'Asia/Kolkata' },
                    cardinality: 8760,
                    uniqueValues: [],
                },
                {
                    dimensionId: 'operation_type',
                    name: 'Operation Type',
                    type: 'categorical',
                    hierarchy: ['category', 'type'],
                    attributes: {},
                    cardinality: 15,
                    uniqueValues: [
                        { value: 'order_processing', label: 'Order Processing', frequency: 40 },
                        { value: 'meal_preparation', label: 'Meal Preparation', frequency: 30 },
                        { value: 'delivery', label: 'Delivery', frequency: 20 },
                        { value: 'customer_service', label: 'Customer Service', frequency: 10 },
                    ],
                },
            ],
            measures: [
                {
                    measureId: 'efficiency_score',
                    name: 'Efficiency Score',
                    dataType: 'percentage',
                    aggregationType: 'avg',
                    unit: 'percentage',
                    format: '#0.0%',
                    businessRules: ['Range 0-100%'],
                },
                {
                    measureId: 'processing_time',
                    name: 'Processing Time',
                    dataType: 'decimal',
                    aggregationType: 'avg',
                    unit: 'minutes',
                    format: '#0.0',
                    businessRules: ['Non-negative', 'Reasonable upper bounds'],
                },
            ],
            factTable: 'operational_facts',
            refreshFrequency: 'real_time',
            lastRefresh: new Date(),
            dataQuality: {
                completeness: 99.2,
                accuracy: 98.1,
                consistency: 98.8,
                timeliness: 99.9,
                validity: 97.5,
                overall: 98.7,
            },
            size: {
                rows: 5000000,
                compressed: 1024 * 1024 * 200,
                uncompressed: 1024 * 1024 * 600,
            },
            partitioning: {
                strategy: 'time',
                columns: ['operation_timestamp'],
                partitions: 365,
            },
        };
        this.cubeCache.set('operational_analytics', operationalCube);
    }
    initializeETLProcesses() {
        const revenueETL = {
            processId: 'revenue_etl',
            name: 'Revenue Data ETL Pipeline',
            description: 'Extract, transform, and load revenue data from transactional systems',
            type: 'full_etl',
            source: {
                type: 'database',
                connection: 'primary_db',
                schema: 'public',
                tables: ['payments', 'orders', 'subscriptions'],
                query: `
          SELECT p.id, p.amount, p.status, p.created_at,
                 o.id as order_id, o.quantity,
                 u.school_id, s.name as school_name, s.tier
          FROM payments p
          JOIN orders o ON p.order_id = o.id
          JOIN users u ON o.user_id = u.id
          JOIN schools s ON u.school_id = s.id
          WHERE p.created_at >= NOW() - INTERVAL '1 day'
        `,
            },
            transformations: [
                {
                    step: 1,
                    type: 'validate',
                    description: 'Validate payment status and amounts',
                    logic: 'status IN (completed, pending) AND amount > 0',
                    inputColumns: ['status', 'amount'],
                    outputColumns: ['status', 'amount', 'is_valid'],
                },
                {
                    step: 2,
                    type: 'calculate',
                    description: 'Calculate derived metrics',
                    logic: 'revenue_per_student = amount / quantity',
                    inputColumns: ['amount', 'quantity'],
                    outputColumns: ['revenue_per_student'],
                },
                {
                    step: 3,
                    type: 'aggregate',
                    description: 'Aggregate by school and time dimensions',
                    logic: 'GROUP BY school_id, DATE(created_at)',
                    inputColumns: ['school_id', 'created_at', 'amount'],
                    outputColumns: ['school_id', 'date', 'total_revenue', 'order_count'],
                },
            ],
            target: {
                type: 'warehouse',
                connection: 'analytics_warehouse',
                schema: 'analytics',
                table: 'revenue_facts',
                mode: 'merge',
            },
            schedule: {
                frequency: 'hourly',
                time: '00:15',
                timezone: 'Asia/Kolkata',
                dependencies: ['data_validation_etl'],
            },
            monitoring: {
                lastRun: new Date(),
                status: 'success',
                duration: 180,
                recordsProcessed: 15420,
                recordsInserted: 15200,
                recordsUpdated: 180,
                recordsRejected: 40,
                errorRate: 0.26,
                performanceMetrics: {
                    throughput: 85.7,
                    memoryUsage: 256,
                    cpuUsage: 65,
                },
            },
            dataQuality: {
                rules: [
                    {
                        ruleId: 'revenue_positive',
                        name: 'Revenue must be positive',
                        type: 'validity',
                        expression: 'amount > 0',
                        threshold: 99.5,
                        action: 'reject',
                        status: 'passed',
                        violationCount: 2,
                    },
                    {
                        ruleId: 'payment_status_valid',
                        name: 'Payment status must be valid',
                        type: 'validity',
                        expression: 'status IN (completed, pending, failed)',
                        threshold: 100,
                        action: 'reject',
                        status: 'passed',
                        violationCount: 0,
                    },
                ],
                overallScore: 98.7,
                trend: 'stable',
            },
        };
        this.etlProcesses.set('revenue_etl', revenueETL);
    }
    async aggregateData(dimensions, measures, timeGranularity, dateRange, filters = {}, includeComparisons = true) {
        const startTime = Date.now();
        const isCriticalOperation = this.isCriticalOperation(dimensions, measures);
        if (isCriticalOperation) {
            const timeoutPromise = new Promise((_, reject) => {
                setTimeout(() => reject(new Error('CRITICAL_OPERATION_TIMEOUT')), 450);
            });
            const operationPromise = this.performAggregation(dimensions, measures, timeGranularity, dateRange, filters, includeComparisons, startTime);
            return Promise.race([operationPromise, timeoutPromise]);
        }
        return this.performAggregation(dimensions, measures, timeGranularity, dateRange, filters, includeComparisons, startTime);
    }
    isCriticalOperation(dimensions, measures) {
        const criticalMeasures = ['revenue', 'orders'];
        const criticalDimensions = ['time', 'school'];
        return (measures.some(m => criticalMeasures.includes(m)) &&
            dimensions.every(d => criticalDimensions.includes(d)) &&
            dimensions.length <= 2 &&
            measures.length <= 3);
    }
    async performAggregation(dimensions, measures, timeGranularity, dateRange, filters, includeComparisons, startTime) {
        this.logger.info('Starting data aggregation', {
            dimensions,
            measures,
            timeGranularity,
            dateRange,
            includeComparisons,
        });
        const cacheKey = this.generateCacheKey(dimensions, measures, timeGranularity, dateRange, filters, includeComparisons);
        try {
            const cachedResult = await this.getCachedAggregation(cacheKey);
            if (cachedResult) {
                await this.monitoring.recordCacheOperation('hit');
                await this.monitoring.recordApiRequest(Date.now() - startTime, false);
                this.logger.info('Data aggregation served from Redis cache', {
                    cacheKey,
                    processingTime: Date.now() - startTime,
                });
                return cachedResult;
            }
            await this.monitoring.recordCacheOperation('miss');
            const results = await this.executeAggregationQuery(dimensions, measures, timeGranularity, dateRange, filters);
            const summaryStatistics = this.calculateSummaryStatistics(results, dimensions, measures);
            let comparisons;
            if (includeComparisons) {
                comparisons = await this.generateComparisons(dimensions, measures, timeGranularity, dateRange, filters);
            }
            const insights = await this.generateDataInsights(results, dimensions, measures);
            const aggregatedData = {
                aggregationId: `agg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
                generatedAt: new Date(),
                query: {
                    dimensions,
                    measures,
                    filters,
                    timeRange: {
                        start: dateRange.startDate,
                        end: dateRange.endDate,
                        granularity: timeGranularity,
                    },
                },
                results,
                summaryStatistics,
                comparisons,
                insights,
            };
            const processingTime = Date.now() - startTime;
            this.logger.info('Data aggregation completed', {
                aggregationId: aggregatedData.aggregationId,
                processingTime,
                resultCount: results.length,
                insightCount: insights.length,
            });
            await this.monitoring.recordApiRequest(processingTime, false);
            await this.monitoring.updateMemoryMetrics();
            await this.cacheAggregationResult(cacheKey, aggregatedData);
            this.aggregationCache.set(aggregatedData.aggregationId, {
                data: aggregatedData,
                timestamp: Date.now(),
                accessCount: 1,
            });
            return aggregatedData;
        }
        catch (error) {
            const processingTime = Date.now() - startTime;
            await this.monitoring.recordApiRequest(processingTime, true);
            this.logger.error('Data aggregation failed', undefined, {
                errorMessage: error instanceof Error ? error.message : String(error),
                dimensions,
                measures,
                timeGranularity,
            });
            throw error;
        }
    }
    async executeAggregationQuery(dimensions, measures, timeGranularity, dateRange, filters) {
        const prismaClient = this.database.client;
        const results = [];
        if (measures.includes('revenue') || measures.includes('orders')) {
            const query = this.buildOptimizedAggregationQuery(dimensions, measures, timeGranularity, dateRange, filters);
            const params = this.getQueryParameters(dateRange, filters);
            const rawResults = await prismaClient.$queryRawUnsafe(query, ...params);
            for (const row of rawResults) {
                const dimensionValues = {};
                const measureValues = {};
                for (const dimension of dimensions) {
                    if (row.hasOwnProperty(dimension)) {
                        dimensionValues[dimension] = row[dimension];
                    }
                }
                for (const measure of measures) {
                    if (row.hasOwnProperty(measure)) {
                        measureValues[measure] = Number(row[measure]);
                    }
                }
                if (measures.includes('satisfaction')) {
                    measureValues.satisfaction = 4.2 + Math.random() * 0.8;
                }
                if (measures.includes('efficiency')) {
                    measureValues.efficiency = 75 + Math.random() * 20;
                }
                results.push({
                    dimensionValues,
                    measureValues,
                    metadata: {
                        recordCount: Number(row.record_count || 1),
                        confidence: 0.95,
                        dataQuality: 0.92,
                    },
                });
            }
        }
        return results;
    }
    buildOptimizedAggregationQuery(dimensions, measures, timeGranularity, dateRange, filters) {
        const selectParts = [];
        const groupByParts = [];
        const whereParts = [];
        for (const dimension of dimensions) {
            switch (dimension) {
                case 'time':
                    const timeFormat = this.getTimeFormatSQL(timeGranularity);
                    selectParts.push(`${timeFormat} as time`);
                    groupByParts.push(timeFormat);
                    break;
                case 'school':
                    selectParts.push('s.name as school');
                    groupByParts.push('s.id');
                    break;
                case 'region':
                    selectParts.push('s.city as region');
                    groupByParts.push('s.city');
                    break;
                case 'subscription_tier':
                    selectParts.push('s.subscription_tier as subscription_tier');
                    groupByParts.push('s.subscription_tier');
                    break;
                case 'user_type':
                    selectParts.push('u.role as user_type');
                    groupByParts.push('u.role');
                    break;
            }
        }
        if (measures.includes('revenue')) {
            selectParts.push('SUM(p.amount) as revenue');
        }
        if (measures.includes('orders')) {
            selectParts.push('COUNT(DISTINCT o.id) as orders');
        }
        if (measures.includes('students')) {
            selectParts.push('COUNT(DISTINCT o.student_id) as students');
        }
        selectParts.push('COUNT(*) as record_count');
        whereParts.push('p.created_at >= ?', 'p.created_at <= ?', "p.status = 'completed'");
        if (filters.schoolId) {
            whereParts.push('u.school_id = ?');
        }
        if (filters.minAmount) {
            whereParts.push('p.amount >= ?');
        }
        const selectClause = selectParts.join(', ');
        const groupByClause = groupByParts.length > 0 ? `GROUP BY ${groupByParts.join(', ')}` : '';
        const whereClause = whereParts.length > 0 ? `WHERE ${whereParts.join(' AND ')}` : '';
        return `
      SELECT ${selectClause}
      FROM payments p
      JOIN orders o ON p.order_id = o.id
      JOIN users u ON o.user_id = u.id
      JOIN schools s ON u.school_id = s.id
      ${whereClause}
      ${groupByClause}
      ORDER BY ${groupByParts.length > 0 ? groupByParts[0] : '1'}
      LIMIT 1000
    `;
    }
    getTimeFormatSQL(granularity) {
        switch (granularity) {
            case 'hour':
                return "strftime('%Y-%m-%d-%H', p.created_at)";
            case 'day':
                return "strftime('%Y-%m-%d', p.created_at)";
            case 'week':
                return "strftime('%Y-%W', p.created_at)";
            case 'month':
                return "strftime('%Y-%m', p.created_at)";
            case 'quarter':
                return "strftime('%Y-', p.created_at) || ((strftime('%m', p.created_at) - 1) / 3 + 1)";
            case 'year':
                return "strftime('%Y', p.created_at)";
            default:
                return "strftime('%Y-%m-%d', p.created_at)";
        }
    }
    groupDataByDimensions(data, dimensions, timeGranularity) {
        const grouped = {};
        if (!data) {
            return grouped;
        }
        for (const item of data) {
            const key = this.generateDimensionKey(item, dimensions, timeGranularity);
            if (!grouped[key]) {
                grouped[key] = [];
            }
            grouped[key].push(item);
        }
        return grouped;
    }
    generateDimensionKey(item, dimensions, timeGranularity) {
        const keyParts = [];
        for (const dimension of dimensions) {
            switch (dimension) {
                case 'time':
                    const date = new Date(item.createdAt);
                    keyParts.push(this.formatTimeByGranularity(date, timeGranularity));
                    break;
                case 'school':
                    keyParts.push(String(item.order?.user?.school?.id || 'unknown'));
                    break;
                case 'region':
                    keyParts.push(String(item.order?.user?.school?.region || 'unknown'));
                    break;
                case 'subscription_tier':
                    keyParts.push(String(item.order?.user?.school?.tier || 'unknown'));
                    break;
                case 'user_type':
                    keyParts.push(String(item.order?.user?.role || 'unknown'));
                    break;
                default:
                    keyParts.push('unknown');
            }
        }
        return keyParts.join('|');
    }
    parseDimensionKey(key, dimensions) {
        const parts = key.split('|');
        const values = {};
        dimensions.forEach((dimension, index) => {
            values[dimension] = parts[index] || 'unknown';
        });
        return values;
    }
    formatTimeByGranularity(date, granularity) {
        switch (granularity) {
            case 'hour':
                return `${date.getFullYear()}-${String(date.getMonth() + 1).padStart(2, '0')}-${String(date.getDate()).padStart(2, '0')}-${String(date.getHours()).padStart(2, '0')}`;
            case 'day':
                return `${date.getFullYear()}-${String(date.getMonth() + 1).padStart(2, '0')}-${String(date.getDate()).padStart(2, '0')}`;
            case 'week':
                const weekStart = new Date(date);
                weekStart.setDate(date.getDate() - date.getDay());
                return `${weekStart.getFullYear()}-W${String(Math.ceil((weekStart.getTime() - new Date(weekStart.getFullYear(), 0, 1).getTime()) / (7 * 24 * 60 * 60 * 1000))).padStart(2, '0')}`;
            case 'month':
                return `${date.getFullYear()}-${String(date.getMonth() + 1).padStart(2, '0')}`;
            case 'quarter':
                return `${date.getFullYear()}-Q${Math.ceil((date.getMonth() + 1) / 3)}`;
            case 'year':
                return `${date.getFullYear()}`;
            default:
                return date.toISOString().split('T')[0];
        }
    }
    buildFilters(filters) {
        const whereClause = {};
        if (filters.schoolId) {
            whereClause.order = {
                user: {
                    schoolId: filters.schoolId,
                },
            };
        }
        if (filters.minAmount) {
            whereClause.amount = {
                gte: filters.minAmount,
            };
        }
        return whereClause;
    }
    getQueryParameters(dateRange, filters) {
        const params = [dateRange.startDate.toISOString(), dateRange.endDate.toISOString()];
        if (filters.schoolId) {
            params.push(filters.schoolId);
        }
        if (filters.minAmount) {
            params.push(filters.minAmount);
        }
        return params;
    }
    calculateSummaryStatistics(results, dimensions, measures) {
        const totalRecords = results.length;
        const dimensionStats = {};
        const measureStats = {};
        for (const dimension of dimensions) {
            const values = results.map(r => r.dimensionValues[dimension]);
            const uniqueValues = new Set(values);
            const nullCount = values.filter(v => v === null || v === undefined || v === 'unknown').length;
            const distribution = Array.from(uniqueValues)
                .map(value => {
                const count = values.filter(v => v === value).length;
                return {
                    value,
                    count,
                    percentage: (count / totalRecords) * 100,
                };
            })
                .sort((a, b) => b.count - a.count);
            dimensionStats[dimension] = {
                uniqueValues: uniqueValues.size,
                nullCount,
                distribution,
            };
        }
        for (const measure of measures) {
            const values = results.map(r => r.measureValues[measure]).filter(v => v !== undefined);
            if (values.length > 0) {
                const sum = values.reduce((s, v) => s + v, 0);
                const avg = sum / values.length;
                const min = Math.min(...values);
                const max = Math.max(...values);
                const sortedValues = values.sort((a, b) => a - b);
                const variance = values.reduce((s, v) => s + Math.pow(v - avg, 2), 0) / values.length;
                const stdDev = Math.sqrt(variance);
                measureStats[measure] = {
                    sum,
                    avg,
                    min,
                    max,
                    stdDev,
                    percentiles: {
                        p25: sortedValues[Math.floor(values.length * 0.25)],
                        p50: sortedValues[Math.floor(values.length * 0.5)],
                        p75: sortedValues[Math.floor(values.length * 0.75)],
                        p90: sortedValues[Math.floor(values.length * 0.9)],
                        p95: sortedValues[Math.floor(values.length * 0.95)],
                    },
                };
            }
        }
        return {
            totalRecords,
            dimensions: dimensionStats,
            measures: measureStats,
        };
    }
    async generateComparisons(dimensions, measures, timeGranularity, dateRange, filters) {
        const daysDiff = Math.ceil((dateRange.endDate.getTime() - dateRange.startDate.getTime()) / (1000 * 60 * 60 * 24));
        const previousStartDate = new Date(dateRange.startDate.getTime() - daysDiff * 24 * 60 * 60 * 1000);
        const previousEndDate = new Date(dateRange.startDate.getTime());
        const previousResults = await this.executeAggregationQuery(dimensions, measures, timeGranularity, { startDate: previousStartDate, endDate: previousEndDate }, filters);
        const change = {};
        const changePercentage = {};
        for (const measure of measures) {
            const currentTotal = this.calculateMeasureTotal(await this.executeAggregationQuery(dimensions, measures, timeGranularity, dateRange, filters), measure);
            const previousTotal = this.calculateMeasureTotal(previousResults, measure);
            change[measure] = currentTotal - previousTotal;
            changePercentage[measure] =
                previousTotal > 0 ? ((currentTotal - previousTotal) / previousTotal) * 100 : 0;
        }
        return {
            previousPeriod: {
                change,
                changePercentage,
            },
            benchmark: {
                industry: measures.reduce((acc, measure) => ({ ...acc, [measure]: 1000000 }), {}),
                target: measures.reduce((acc, measure) => ({ ...acc, [measure]: 1200000 }), {}),
                variance: measures.reduce((acc, measure) => ({ ...acc, [measure]: -15.5 }), {}),
            },
        };
    }
    calculateMeasureTotal(results, measure) {
        return results.reduce((sum, result) => sum + (result.measureValues[measure] || 0), 0);
    }
    async generateDataInsights(results, dimensions, measures) {
        const insights = [];
        if (dimensions.includes('time') && results.length > 2) {
            const timeResults = results.sort((a, b) => new Date(a.dimensionValues.time).getTime() -
                new Date(b.dimensionValues.time).getTime());
            for (const measure of measures) {
                const values = timeResults.map(r => r.measureValues[measure]).filter(v => v !== undefined);
                if (values.length >= 3) {
                    const trend = this.calculateTrend(values);
                    if (Math.abs(trend.slope) > 0.1) {
                        insights.push({
                            type: 'trend',
                            description: `${measure} shows ${trend.slope > 0 ? 'upward' : 'downward'} trend with ${Math.abs(trend.slope * 100).toFixed(1)}% rate`,
                            significance: Math.min(1, Math.abs(trend.slope) * 2),
                            confidence: trend.rSquared,
                            recommendation: trend.slope > 0
                                ? `Continue strategies driving ${measure} growth`
                                : `Address factors causing ${measure} decline`,
                        });
                    }
                }
            }
        }
        for (const measure of measures) {
            const values = results.map(r => r.measureValues[measure]).filter(v => v !== undefined);
            if (values.length > 5) {
                const anomalies = this.detectAnomalies(values);
                if (anomalies.length > 0) {
                    insights.push({
                        type: 'anomaly',
                        description: `Detected ${anomalies.length} anomalies in ${measure} data`,
                        significance: 0.8,
                        confidence: 0.75,
                        recommendation: `Investigate unusual patterns in ${measure} for potential issues or opportunities`,
                    });
                }
            }
        }
        if (measures.length > 1) {
            const correlations = this.calculateCorrelations(results, measures);
            for (const [pair, correlation] of Object.entries(correlations)) {
                if (Math.abs(correlation) > 0.7) {
                    insights.push({
                        type: 'correlation',
                        description: `Strong ${correlation > 0 ? 'positive' : 'negative'} correlation (${(correlation * 100).toFixed(1)}%) between ${pair}`,
                        significance: Math.abs(correlation),
                        confidence: 0.85,
                        recommendation: `Leverage ${pair} relationship for predictive modeling and optimization`,
                    });
                }
            }
        }
        return insights.sort((a, b) => b.significance - a.significance);
    }
    calculateTrend(values) {
        const n = values.length;
        const xValues = Array.from({ length: n }, (_, i) => i);
        const xMean = xValues.reduce((sum, x) => sum + x, 0) / n;
        const yMean = values.reduce((sum, y) => sum + y, 0) / n;
        const numerator = xValues.reduce((sum, x, i) => sum + (x - xMean) * (values[i] - yMean), 0);
        const denominator = xValues.reduce((sum, x) => sum + Math.pow(x - xMean, 2), 0);
        const slope = denominator !== 0 ? numerator / denominator : 0;
        const predicted = xValues.map(x => yMean + slope * (x - xMean));
        const ssRes = values.reduce((sum, y, i) => sum + Math.pow(y - predicted[i], 2), 0);
        const ssTot = values.reduce((sum, y) => sum + Math.pow(y - yMean, 2), 0);
        const rSquared = ssTot > 0 ? 1 - ssRes / ssTot : 0;
        return { slope, rSquared };
    }
    detectAnomalies(values) {
        const mean = values.reduce((sum, v) => sum + v, 0) / values.length;
        const stdDev = Math.sqrt(values.reduce((sum, v) => sum + Math.pow(v - mean, 2), 0) / values.length);
        return values.filter(v => Math.abs(v - mean) > 2.5 * stdDev);
    }
    calculateCorrelations(results, measures) {
        const correlations = {};
        for (let i = 0; i < measures.length; i++) {
            for (let j = i + 1; j < measures.length; j++) {
                const measure1 = measures[i];
                const measure2 = measures[j];
                const values1 = results.map(r => r.measureValues[measure1]).filter(v => v !== undefined);
                const values2 = results.map(r => r.measureValues[measure2]).filter(v => v !== undefined);
                if (values1.length === values2.length && values1.length > 2) {
                    const correlation = this.pearsonCorrelation(values1, values2);
                    correlations[`${measure1}_${measure2}`] = correlation;
                }
            }
        }
        return correlations;
    }
    pearsonCorrelation(x, y) {
        const n = x.length;
        const sumX = x.reduce((sum, val) => sum + val, 0);
        const sumY = y.reduce((sum, val) => sum + val, 0);
        const sumXY = x.reduce((sum, val, i) => sum + val * y[i], 0);
        const sumXX = x.reduce((sum, val) => sum + val * val, 0);
        const sumYY = y.reduce((sum, val) => sum + val * val, 0);
        const numerator = n * sumXY - sumX * sumY;
        const denominator = Math.sqrt((n * sumXX - sumX * sumX) * (n * sumYY - sumY * sumY));
        return denominator !== 0 ? numerator / denominator : 0;
    }
    async processETL(operation, sourceType, processingMode = 'batch') {
        this.logger.info('Starting ETL process', { operation, sourceType, processingMode });
        const processId = `${operation}_${sourceType}_${Date.now()}`;
        const startTime = Date.now();
        try {
            const etlProcess = this.etlProcesses.get('revenue_etl');
            if (!etlProcess) {
                throw new Error('ETL process configuration not found');
            }
            etlProcess.monitoring.status = 'running';
            etlProcess.monitoring.lastRun = new Date();
            let recordsProcessed = 0;
            let recordsInserted = 0;
            let recordsRejected = 0;
            for (const transformation of etlProcess.transformations) {
                this.logger.info('Executing transformation step', {
                    step: transformation.step,
                    type: transformation.type,
                    description: transformation.description,
                });
                const stepRecords = Math.floor(Math.random() * 1000) + 500;
                recordsProcessed += stepRecords;
                const rejectionRate = 0.02;
                const stepRejected = Math.floor(stepRecords * rejectionRate);
                recordsRejected += stepRejected;
                recordsInserted += stepRecords - stepRejected;
                await new Promise(resolve => setTimeout(resolve, 100));
            }
            const duration = (Date.now() - startTime) / 1000;
            etlProcess.monitoring = {
                ...etlProcess.monitoring,
                status: 'success',
                duration,
                recordsProcessed,
                recordsInserted,
                recordsUpdated: Math.floor(recordsInserted * 0.1),
                recordsRejected,
                errorRate: (recordsRejected / recordsProcessed) * 100,
                performanceMetrics: {
                    throughput: recordsProcessed / duration,
                    memoryUsage: 150 + Math.random() * 100,
                    cpuUsage: 40 + Math.random() * 30,
                },
            };
            etlProcess.dataQuality.overallScore = 98.5 - (recordsRejected / recordsProcessed) * 100;
            this.logger.info('ETL process completed successfully', {
                processId,
                duration,
                recordsProcessed,
                recordsInserted,
                recordsRejected,
            });
            return etlProcess;
        }
        catch (error) {
            this.logger.error('ETL process failed', error instanceof Error ? error : new Error(String(error)), {
                processId,
            });
            throw error;
        }
    }
    async analyzeDataLineage(entityId, entityType) {
        this.logger.info('Analyzing data lineage', { entityId, entityType });
        const lineage = {
            entityId,
            entityType: entityType,
            entityName: `Entity_${entityId}`,
            upstream: [
                {
                    entityId: 'payments_table',
                    entityName: 'payments',
                    entityType: 'table',
                    relationship: 'direct',
                    transformations: ['ETL transformation', 'Data validation'],
                    confidence: 0.95,
                },
                {
                    entityId: 'orders_table',
                    entityName: 'orders',
                    entityType: 'table',
                    relationship: 'direct',
                    transformations: ['Join transformation'],
                    confidence: 0.9,
                },
            ],
            downstream: [
                {
                    entityId: 'revenue_dashboard',
                    entityName: 'Revenue Dashboard',
                    entityType: 'dashboard',
                    relationship: 'indirect',
                    usage: 'dashboard',
                    impact: 'high',
                },
                {
                    entityId: 'monthly_report',
                    entityName: 'Monthly Revenue Report',
                    entityType: 'report',
                    relationship: 'direct',
                    usage: 'report',
                    impact: 'critical',
                },
            ],
            metadata: {
                owner: 'Data Engineering Team',
                steward: 'Business Intelligence Team',
                classification: 'internal',
                tags: ['revenue', 'financial', 'business_critical'],
                businessTerms: ['Revenue', 'Sales', 'Income'],
                technicalTerms: ['aggregation', 'fact_table', 'dimension'],
                lastUpdated: new Date(),
            },
            qualityMetrics: {
                completeness: 98.5,
                accuracy: 96.8,
                consistency: 97.2,
                timeliness: 99.1,
                usage: 85.3,
                trust: 92.7,
            },
        };
        return lineage;
    }
    getDataCube(cubeId) {
        return this.cubeCache.get(cubeId) || null;
    }
    listDataCubes() {
        return Array.from(this.cubeCache.values());
    }
    getETLStatus(processId) {
        if (processId) {
            const process = this.etlProcesses.get(processId);
            return process ? [process] : [];
        }
        return Array.from(this.etlProcesses.values());
    }
}
exports.BusinessIntelligenceAggregator = BusinessIntelligenceAggregator;
const businessIntelligenceAggregator = new BusinessIntelligenceAggregator();
exports.businessIntelligenceAggregator = businessIntelligenceAggregator;
const businessIntelligenceHandler = async (event, context) => {
    const logger = logger_service_1.LoggerService.getInstance();
    const requestId = context.awsRequestId;
    try {
        logger.info('Business intelligence request started', {
            requestId,
            method: event.httpMethod,
            path: event.path,
        });
        const authResult = await (0, lambda_auth_middleware_1.authenticateLambda)(event);
        if ('statusCode' in authResult) {
            logger.warn('Authentication failed for business intelligence aggregator', {
                requestId,
                ip: event.requestContext.identity.sourceIp,
            });
            return authResult;
        }
        const { user: authenticatedUser } = authResult;
        if (!authenticatedUser ||
            !['admin', 'super_admin', 'data_analyst', 'business_analyst'].includes(authenticatedUser.role)) {
            logger.warn('Insufficient permissions for business intelligence aggregator', {
                requestId,
                userId: authenticatedUser?.id,
                role: authenticatedUser?.role,
            });
            return (0, response_utils_1.createErrorResponse)('INSUFFICIENT_PERMISSIONS', 'Business intelligence requires analyst level permissions', 403);
        }
        const method = event.httpMethod;
        const pathSegments = event.path.split('/').filter(Boolean);
        const operation = pathSegments[pathSegments.length - 1];
        switch (method) {
            case 'GET':
                const queryParams = {};
                if (event.queryStringParameters) {
                    for (const [key, value] of Object.entries(event.queryStringParameters)) {
                        if (value !== undefined) {
                            queryParams[key] = value;
                        }
                    }
                }
                return await handleGetRequest(operation, queryParams, authenticatedUser, requestId);
            case 'POST':
                return await handlePostRequest(operation, event.body, authenticatedUser, requestId);
            default:
                return (0, response_utils_1.createErrorResponse)('METHOD_NOT_ALLOWED', `Method ${method} not allowed`, 405);
        }
    }
    catch (error) {
        logger.error('Business intelligence request failed', undefined, {
            errorMessage: error instanceof Error ? error.message : String(error),
            requestId,
            stack: error instanceof Error ? error.stack : undefined,
        });
        return (0, response_utils_1.handleError)(error, 'Business intelligence operation failed');
    }
};
exports.businessIntelligenceHandler = businessIntelligenceHandler;
async function handleGetRequest(operation, queryParams, authenticatedUser, requestId) {
    const logger = logger_service_1.LoggerService.getInstance();
    switch (operation) {
        case 'cubes':
            const cubes = businessIntelligenceAggregator.listDataCubes();
            return (0, response_utils_1.createSuccessResponse)({
                message: 'Data cubes retrieved successfully',
                data: cubes,
            });
        case 'cube':
            const { cubeId } = queryParams;
            if (!cubeId) {
                return (0, response_utils_1.createErrorResponse)('MISSING_PARAMETER', 'Cube ID is required', 400);
            }
            const cube = businessIntelligenceAggregator.getDataCube(cubeId);
            if (!cube) {
                return (0, response_utils_1.createErrorResponse)('CUBE_NOT_FOUND', 'Data cube not found', 404);
            }
            return (0, response_utils_1.createSuccessResponse)({
                message: 'Data cube retrieved successfully',
                data: cube,
            });
        case 'etl-status':
            const { processId } = queryParams;
            const etlProcesses = businessIntelligenceAggregator.getETLStatus(processId);
            return (0, response_utils_1.createSuccessResponse)({
                message: 'ETL status retrieved successfully',
                data: etlProcesses,
            });
        case 'lineage':
            const { entityId } = queryParams;
            const { entityType } = queryParams;
            if (!entityId || !entityType) {
                return (0, response_utils_1.createErrorResponse)('MISSING_PARAMETERS', 'Entity ID and type are required', 400);
            }
            const lineage = await businessIntelligenceAggregator.analyzeDataLineage(entityId, entityType);
            return (0, response_utils_1.createSuccessResponse)({
                message: 'Data lineage analyzed successfully',
                data: lineage,
            });
        case 'aggregate':
            try {
                const query = dataAggregationQuerySchema.parse(queryParams);
                const aggregatedData = await businessIntelligenceAggregator.aggregateData(query.dimensions, query.measures, query.timeGranularity, {
                    startDate: new Date(query.dateRange.startDate),
                    endDate: new Date(query.dateRange.endDate),
                }, query.filters, query.includeComparisons);
                return (0, response_utils_1.createSuccessResponse)({
                    message: 'Data aggregated successfully',
                    data: aggregatedData,
                });
            }
            catch (error) {
                logger.error('Data aggregation failed', undefined, {
                    errorMessage: error instanceof Error ? error.message : String(error),
                    requestId,
                    queryParams,
                });
                if (error instanceof zod_1.z.ZodError) {
                    return (0, response_utils_1.validationErrorResponse)(error.issues.map(e => `${e.path.join('.')}: ${e.message}`).join(', '), error.issues);
                }
                this.logger.error('Data aggregation failed', undefined, {
                    errorMessage: error instanceof Error ? error.message : String(error),
                    requestId,
                    queryParams,
                });
                throw error;
            }
        default:
            return (0, response_utils_1.createErrorResponse)('UNKNOWN_OPERATION', 'Unknown operation', 400);
    }
}
async function handlePostRequest(operation, requestBody, authenticatedUser, requestId) {
    const logger = logger_service_1.LoggerService.getInstance();
    if (!requestBody) {
        return (0, response_utils_1.createErrorResponse)('MISSING_BODY', 'Request body is required', 400);
    }
    const body = JSON.parse(requestBody);
    switch (operation) {
        case 'process-etl':
            try {
                const etlRequest = dataProcessingSchema.parse(body);
                const etlResult = await businessIntelligenceAggregator.processETL(etlRequest.operation, etlRequest.sourceType, etlRequest.processingMode);
                return (0, response_utils_1.createSuccessResponse)({
                    message: 'ETL process executed successfully',
                    data: etlResult,
                });
            }
            catch (error) {
                logger.error('ETL processing failed', undefined, {
                    errorMessage: error instanceof Error ? error.message : String(error),
                    requestId,
                    body,
                });
                if (error instanceof zod_1.z.ZodError) {
                    return (0, response_utils_1.validationErrorResponse)(error.issues.map(e => `${e.path.join('.')}: ${e.message}`).join(', '), error.issues);
                }
                this.logger.error('ETL processing failed', undefined, {
                    errorMessage: error instanceof Error ? error.message : String(error),
                    requestId,
                    body,
                });
                throw error;
            }
        default:
            return (0, response_utils_1.createErrorResponse)('UNKNOWN_OPERATION', 'Unknown operation', 400);
    }
}
exports.handler = exports.businessIntelligenceHandler;
//# sourceMappingURL=business-intelligence-aggregator.js.map